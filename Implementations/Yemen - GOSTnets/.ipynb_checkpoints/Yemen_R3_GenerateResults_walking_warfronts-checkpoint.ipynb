{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peartree version: 0.6.1 \n",
      "networkx version: 2.3 \n",
      "matplotlib version: 3.0.3 \n",
      "osmnx version: 0.9 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets')\n",
    "sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST')\n",
    "import GOSTnet as gn\n",
    "import importlib\n",
    "import geopandas as gpd\n",
    "import rasterio as rt\n",
    "from rasterio import features\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from shapely.geometry import box, Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 35636, {'Wkt': 'LINESTRING (44.2165745 15.3646484, 44.2167032 15.3659494, 44.2167203 15.3663506, 44.2167763 15.3673335, 44.2167973 15.3681592)', 'id': 24386, 'infra_type': 'secondary', 'osm_id': 108470243, 'country': 'YEM', 'key': 'edge_24386', 'length': 389.3715222365589, 'Type': 'legitimate', 'time': 28.03474960103224, 'mode': 'drive', 'ID': 1, 'time_November1st': 28.03474960103224, 'MOD_November1st': 'normal', 'time_November8th': 28.03474960103224, 'MOD_November8th': 'normal', 'time_November14th': 28.03474960103224, 'MOD_November14th': 'normal', 'time_November25th': 28.03474960103224, 'MOD_November25th': 'normal', 'time_December17th': 28.03474960103224, 'MOD_December17th': 'normal', 'time_January24th': 28.03474960103224, 'MOD_January24th': 'normal'})\n"
     ]
    }
   ],
   "source": [
    "gn.example_edge(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "walking = 0 # set to 1 for walking\n",
    "conflict = 1 # set to 1 to prevent people from crossing warfronts\n",
    "zonal_stats = 1 # set to 1 to produce summary zonal stats layer\n",
    "facility_type = 'ALL'   # Options: 'HOS' or 'PHC' or 'ALL'\n",
    "year = 2018   # default = 2018\n",
    "service_index = 8 # Set to 0 for all services / access to hospitals\n",
    "\n",
    "services = ['ALL',\n",
    "            'Antenatal',\n",
    "            'BEmONC',\n",
    "            'CEmONC',\n",
    "            'Under_5',\n",
    "            'Emergency_Surgery',\n",
    "            'Immunizations',\n",
    "            'Malnutrition',\n",
    "            'Int_Outreach']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All-Destination OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\graphtool'\n",
    "basepth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen'\n",
    "util_path = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\util_files'\n",
    "srtm_pth = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\SRTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files will have name:  driving_24th_HERAMS_ALL_Int_Outreach_ConflictAdj_2018\n",
      "network:  G_salty_time_conflict_adj.pickle\n",
      "OD Matrix:  output_Jan24th_driving.csv\n",
      "Conflict setting:  ConflictAdj\n"
     ]
    }
   ],
   "source": [
    "if walking == 1:\n",
    "    type_tag = 'walking'\n",
    "    net_name = r'walk_graph.pickle'\n",
    "else:\n",
    "    type_tag = 'driving'\n",
    "    net_name = r'G_salty_time_conflict_adj.pickle'\n",
    "    \n",
    "if conflict == 1: \n",
    "    conflict_tag = 'ConflictAdj'\n",
    "else:\n",
    "    conflict_tag = 'NoConflict'\n",
    "\n",
    "OD_pth = pth\n",
    "\n",
    "OD_name = r'output_Jan24th_%s.csv' % type_tag\n",
    "net_pth = pth\n",
    "\n",
    "WGS = {'init':'epsg:4326'}\n",
    "measure_crs = {'init':'epsg:32638'}\n",
    "\n",
    "subset = r'%s_24th_HERAMS_%s_%s_%s_%s' % (type_tag, facility_type, services[service_index], conflict_tag, year)\n",
    "print(\"Output files will have name: \", subset)\n",
    "print(\"network: \",net_name)\n",
    "print(\"OD Matrix: \",OD_name)\n",
    "print(\"Conflict setting: \",conflict_tag)\n",
    "                                            \n",
    "offroad_speed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD = pd.read_csv(os.path.join(OD_pth, OD_name))\n",
    "OD = OD.rename(columns = {'Unnamed: 0':'O_ID'})\n",
    "OD = OD.set_index('O_ID')\n",
    "OD = OD.replace([np.inf, -np.inf], np.nan)\n",
    "OD_original = OD.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Subset to Accepted Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acceptable_df = pd.read_csv(os.path.join(OD_pth, 'HeRAMS 2018 April_snapped.csv'))\n",
    "\n",
    "# Adjust for facility type\n",
    "if facility_type == 'HOS':\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df['Health Facility Type Coded'].isin(['1',1])]\n",
    "elif facility_type == 'PHC':\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df['Health Facility Type Coded'].isin([2,'2',3,'3'])]\n",
    "elif facility_type == 'ALL':\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError('unacceptable facility_type entry!')\n",
    "\n",
    "# Adjust for functionality in a given year\n",
    "acceptable_df = acceptable_df.loc[acceptable_df['Functioning %s' % year].isin(['1','2',1,2])]\n",
    "\n",
    "# Adjust for availability of service\n",
    "\n",
    "SERVICE_DICT = {'Antenatal_2018':'ANC 2018',\n",
    "               'Antenatal_2016':'Antenatal Care (P422) 2016',\n",
    "               'BEmONC_2018':'Basic emergency obstetric care 2018',\n",
    "               'BEmONC_2016':'Basic Emergency Obsteteric Care (P424) 2016',\n",
    "               'CEmONC_2018':'Comprehensive emergency obstetric care 2018',\n",
    "               'CEmONC_2016':'Comprehensive Emergency Obstetric Care (S424) 2016',\n",
    "               'Under_5_2018':'Under 5 clinics 2018',\n",
    "               'Under_5_2016':'Under-5 clinic services (P23) 2016',\n",
    "               'Emergency_Surgery_2018':'Emergency and elective surgery 2018',\n",
    "               'Emergency_Surgery_2016':'Emergency and Elective Surgery (S14) 2016',\n",
    "               'Immunizations_2018':'EPI 2018',\n",
    "               'Immunizations_2016':'EPI (P21a) 2016',\n",
    "               'Malnutrition_2018':'Malnutrition services 2018',\n",
    "               'Malnutrition_2016':'Malnutrition services (P25) 2016',\n",
    "               'Int_Outreach_2018':'Integrated outreach (IMCI+EPI+ANC+Nutrition_Services) 2018',\n",
    "               'Int_Outreach_2016':'Integrated Outreach (P22) 2016'}\n",
    "\n",
    "if service_index == 0:\n",
    "    pass\n",
    "else:\n",
    "    acceptable_df = acceptable_df.loc[acceptable_df[SERVICE_DICT['%s_%s' % (services[service_index],year)]].isin(['1',1])]\n",
    "\n",
    "len(acceptable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36624, 3824)\n",
      "(36624, 2182)\n"
     ]
    }
   ],
   "source": [
    "acceptable_df['geometry'] = acceptable_df['geometry'].apply(loads)\n",
    "acceptable_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "accepted_facilities = list(set(list(acceptable_df.NN)))\n",
    "accepted_facilities_str = [str(i) for i in accepted_facilities]\n",
    "OD = OD_original[accepted_facilities_str]\n",
    "acceptable_df.to_csv(os.path.join(basepth,'output_layers','Round 3','%s.csv' % subset))\n",
    "print(OD_original.shape)\n",
    "print(OD.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to add elevation to a point GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elevation(df, x, y, srtm_pth):\n",
    "    # walk all tiles, find path\n",
    "    \n",
    "    tiles = []\n",
    "    for root, folder, files in os.walk(os.path.join(srtm_pth,'high_res')):\n",
    "        for f in files:\n",
    "            if f[-3:] == 'hgt':\n",
    "                tiles.append(f[:-4])\n",
    "\n",
    "    # load dictionary of tiles\n",
    "    arrs = {}\n",
    "    for t in tiles:\n",
    "        arrs[t] = rt.open(srtm_pth+r'\\high_res\\{}.hgt\\{}.hgt'.format(t, t), 'r')\n",
    "\n",
    "    # assign a code\n",
    "    uniques = []\n",
    "    df['code'] = 'placeholder'\n",
    "    def tile_code(z):\n",
    "        E = str(z[x])[:2]\n",
    "        N = str(z[y])[:2]\n",
    "        return 'N{}E0{}'.format(N, E)\n",
    "    df['code'] = df.apply(lambda z: tile_code(z), axis = 1)\n",
    "    unique_codes = list(set(df['code'].unique()))\n",
    "    \n",
    "    z = {}\n",
    "    # Match on High Precision Elevation\n",
    "    property_name = 'elevation'\n",
    "    for code in unique_codes:\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        df2 = df2.loc[df2['code'] == code]\n",
    "        dataset = arrs[code]\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in df2.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "\n",
    "        # generate new dictionary of {node ID: raster values}\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "        \n",
    "    elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "    elev_df.columns = ['elevation']\n",
    "    \n",
    "    missing = elev_df.copy()\n",
    "    missing = missing.loc[missing.elevation < 0]\n",
    "    if len(missing) > 0:\n",
    "        missing_df = df.copy()\n",
    "        missing_df = missing_df.loc[missing.index]\n",
    "        low_res_tifpath = os.path.join(srtm_pth, 'clipped', 'clipped_e20N40.tif')\n",
    "        dataset = rt.open(low_res_tifpath, 'r')\n",
    "        b = dataset.bounds\n",
    "        datasetBoundary = box(b[0], b[1], b[2], b[3])\n",
    "        selKeys = []\n",
    "        selPts = []\n",
    "        for index, row in missing_df.iterrows():\n",
    "            if Point(row[x], row[y]).intersects(datasetBoundary):\n",
    "                selPts.append((row[x],row[y]))\n",
    "                selKeys.append(index)\n",
    "        raster_values = list(dataset.sample(selPts))\n",
    "        raster_values = [x[0] for x in raster_values]\n",
    "        z.update(zip(selKeys, raster_values))\n",
    "\n",
    "        elev_df = pd.DataFrame.from_dict(z, orient='index')\n",
    "        elev_df.columns = ['elevation']\n",
    "    df['point_elev'] = elev_df['elevation']\n",
    "    df = df.drop('code', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to convert distances to walk times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_walktimes(df, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = 6, min_speed = 0.1):\n",
    "    # Tobler's hiking function: https://en.wikipedia.org/wiki/Tobler%27s_hiking_function\n",
    "    def speed(incline_ratio, max_speed):\n",
    "        walkspeed = max_speed * np.exp(-3.5 * abs(incline_ratio + 0.05)) \n",
    "        return walkspeed\n",
    "\n",
    "    speeds = {}\n",
    "    times = {}\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        if data[dist] > 0:\n",
    "            delta_elevation = data[end] - data[start]\n",
    "            incline_ratio = delta_elevation / data[dist]\n",
    "            speed_kmph = speed(incline_ratio = incline_ratio, max_speed = max_walkspeed)\n",
    "            speed_kmph = max(speed_kmph, min_speed)\n",
    "            speeds[index] = (speed_kmph)\n",
    "            times[index] = (data[dist] / 1000 * 3600 / speed_kmph)\n",
    "\n",
    "    speed_df = pd.DataFrame.from_dict(speeds, orient = 'index')\n",
    "    time_df = pd.DataFrame.from_dict(times, orient = 'index')\n",
    "\n",
    "    df['walkspeed'] = speed_df[0]\n",
    "    df['walk_time'] = time_df[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation for destination nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dest_df = acceptable_df[['NN','NN_dist','Latitude','Longitude']]\n",
    "dest_df = add_elevation(dest_df, 'Longitude','Latitude', srtm_pth).set_index('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add elevation from graph nodes (reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(os.path.join(OD_pth, net_name))\n",
    "G_node_df = gn.node_gdf_from_graph(G)\n",
    "G_node_df = add_elevation(G_node_df, 'x', 'y', srtm_pth)\n",
    "match_node_elevs = G_node_df[['node_ID','point_elev']].set_index('node_ID')\n",
    "match_node_elevs.loc[match_node_elevs.point_elev < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match on node elevations for dest_df; calculate travel times to nearest node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_df['node_elev'] = match_node_elevs['point_elev']\n",
    "dest_df = generate_walktimes(dest_df, start = 'node_elev', end = 'point_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "dest_df = dest_df.sort_values(by = 'walk_time', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Walk Time to all travel times in OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_df = dest_df[['walk_time']]\n",
    "dest_df.index = dest_df.index.map(str)\n",
    "\n",
    "d_f = OD.transpose()\n",
    "\n",
    "for i in d_f.columns:\n",
    "    dest_df[i] = d_f[i]\n",
    "    \n",
    "for i in dest_df.columns:\n",
    "    if i == 'walk_time':\n",
    "        pass\n",
    "    else:\n",
    "        dest_df[i] = dest_df[i] + dest_df['walk_time']\n",
    "\n",
    "dest_df = dest_df.drop('walk_time', axis = 1)\n",
    "\n",
    "dest_df = dest_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Shapefile Describing Regions of Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conflict == 1:\n",
    "    conflict_file = r'merged_dists.shp'\n",
    "elif conflict == 0:\n",
    "    conflict_file = r'NoConflict.shp'\n",
    "merged_dists = gpd.read_file(os.path.join(util_path, conflict_file))\n",
    "if merged_dists.crs != {'init':'epsg:4326'}:\n",
    "    merged_dists = merged_dists.to_crs({'init':'epsg:4326'})\n",
    "merged_dists = merged_dists.loc[merged_dists.geometry.type == 'Polygon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor in lines of Control - Import Areas of Control Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersect points with merged districts shapefile, identify relationship\n",
    "\n",
    "def AggressiveSpatialIntersect(points, polygons):\n",
    "    import osmnx as ox\n",
    "    spatial_index = points.sindex\n",
    "    container = {}\n",
    "    cut_geoms = []\n",
    "    for index, row in polygons.iterrows():\n",
    "        polygon = row.geometry\n",
    "        if polygon.area > 0.5:\n",
    "            geometry_cut = ox.quadrat_cut_geometry(polygon, quadrat_width=0.5)\n",
    "            cut_geoms.append(geometry_cut)\n",
    "            print('cutting geometry %s into %s pieces' % (index, len(geometry_cut)))\n",
    "            index_list = []\n",
    "            for P in geometry_cut:\n",
    "                possible_matches_index = list(spatial_index.intersection(P.bounds))\n",
    "                possible_matches = points.iloc[possible_matches_index]\n",
    "                precise_matches = possible_matches[possible_matches.intersects(P)]\n",
    "                if len(precise_matches) > 0:\n",
    "                    index_list.append(precise_matches.index)\n",
    "                flat_list = [item for sublist in index_list for item in sublist]\n",
    "                container[index] = list(set(flat_list))\n",
    "        else:\n",
    "            possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "            possible_matches = points.iloc[possible_matches_index]\n",
    "            precise_matches = possible_matches[possible_matches.intersects(polygon)]\n",
    "            if len(precise_matches) > 0:\n",
    "                container[index] = list(precise_matches.index)\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_node_gdf = gn.node_gdf_from_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "**bag of possible node snapping locations has been successfully generated**\n"
     ]
    }
   ],
   "source": [
    "gdf = graph_node_gdf.copy()\n",
    "gdf = gdf.set_index('node_ID')\n",
    "possible_snap_nodes = AggressiveSpatialIntersect(graph_node_gdf, merged_dists)\n",
    "print('**bag of possible node snapping locations has been successfully generated**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match on network time from origin node (time travelling along network + walking to destination)\n",
    "if year == 2018:\n",
    "    year_raster = 2018\n",
    "elif year == 2016:\n",
    "    year_raster = 2015\n",
    "grid_name = r'origins_1km_%s_snapped.csv' % year_raster\n",
    "grid = pd.read_csv(os.path.join(OD_pth, grid_name))\n",
    "grid = grid.rename({'Unnamed: 0':'PointID'}, axis = 1)\n",
    "grid['geometry'] = grid['geometry'].apply(loads)\n",
    "grid_gdf = gpd.GeoDataFrame(grid, crs = WGS, geometry = 'geometry')\n",
    "grid_gdf = grid_gdf.set_index('PointID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Nearest Node snapping for War"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "bag of possible origins locations has been successfully generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets\\GOSTnet.py:1679: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  G_tree = spatial.KDTree(target_gdf[['x','y']].as_matrix())\n",
      "C:\\Users\\charl\\Documents\\GitHub\\GOST_PublicGoods\\GOSTNets\\GOSTNets\\GOSTnet.py:1681: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  distances, indices = G_tree.query(source_gdf[['x','y']].as_matrix())\n"
     ]
    }
   ],
   "source": [
    "origin_container = AggressiveSpatialIntersect(grid_gdf, merged_dists)\n",
    "print('bag of possible origins locations has been successfully generated')\n",
    "\n",
    "bundle = []\n",
    "for key in origin_container.keys():\n",
    "    origins = origin_container[key]\n",
    "    possible_nodes = graph_node_gdf.loc[possible_snap_nodes[key]]\n",
    "    origin_subset = grid_gdf.loc[origins]\n",
    "    origin_subset_snapped = gn.pandana_snap_points(origin_subset, \n",
    "                                possible_nodes, \n",
    "                                source_crs = 'epsg:4326', \n",
    "                                target_crs = 'epsg:32638', \n",
    "                                add_dist_to_node_col = True)\n",
    "    bundle.append(origin_subset_snapped)\n",
    "\n",
    "grid_gdf_adjusted = pd.concat(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust acceptable destinations for each node for the war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n",
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n"
     ]
    }
   ],
   "source": [
    "gdf = graph_node_gdf.copy()\n",
    "gdf['node_ID'] = gdf['node_ID'].astype('str')\n",
    "gdf = gdf.loc[gdf.node_ID.isin(list(dest_df.columns))]\n",
    "gdf = gdf.set_index('node_ID')\n",
    "\n",
    "dest_container = AggressiveSpatialIntersect(gdf, merged_dists)\n",
    "\n",
    "gdf = graph_node_gdf.copy()\n",
    "gdf = gdf.loc[gdf.node_ID.isin(list(dest_df.index))]\n",
    "gdf = gdf.set_index('node_ID')\n",
    "\n",
    "origin_snap_container = AggressiveSpatialIntersect(gdf, merged_dists)\n",
    "\n",
    "bundle = []\n",
    "for key in origin_snap_container.keys():\n",
    "    origins = origin_snap_container[key]\n",
    "    destinations = dest_container[key]\n",
    "    Q = dest_df[destinations].loc[origins]\n",
    "    Q['min_time'] = Q.min(axis = 1)\n",
    "    Q2 = Q[['min_time']]\n",
    "    bundle.append(Q2)\n",
    "Q3 = pd.concat(bundle)\n",
    "\n",
    "dest_df['min_time'] = Q3['min_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to Normal Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf.rename(columns = {'NN':'O_ID'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on min Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf.reset_index()\n",
    "grid_gdf = grid_gdf.set_index(grid_gdf['O_ID'])\n",
    "grid_gdf['on_network_time'] = dest_df['min_time']\n",
    "grid_gdf = grid_gdf.set_index('PointID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add origin node distance to network - walking time\n",
    "grid = grid_gdf\n",
    "grid = add_elevation(grid, 'Longitude','Latitude', srtm_pth)\n",
    "grid = grid.reset_index()\n",
    "grid = grid.set_index('O_ID')\n",
    "grid['node_elev'] = match_node_elevs['point_elev']\n",
    "grid = grid.set_index('PointID')\n",
    "grid = generate_walktimes(grid, start = 'point_elev', end = 'node_elev', dist = 'NN_dist', max_walkspeed = offroad_speed)\n",
    "grid = grid.rename({'node_elev':'nr_node_on_net_elev', \n",
    "                    'walkspeed':'walkspeed_to_net', \n",
    "                    'walk_time':'walk_time_to_net',\n",
    "                   'NN_dist':'NN_dist_to_net'}, axis = 1)\n",
    "grid['total_time_net'] = grid['on_network_time'] + grid['walk_time_to_net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Direct Walking Time (not using road network), vs. network Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting geometry 56 into 121 pieces\n",
      "cutting geometry 59 into 47 pieces\n",
      "cutting geometry 78 into 236 pieces\n",
      "cutting geometry 79 into 16 pieces\n"
     ]
    }
   ],
   "source": [
    "bundle = []\n",
    "W = graph_node_gdf.copy()\n",
    "W['node_ID'] = W['node_ID'].astype(str)\n",
    "W = W.set_index('node_ID')\n",
    "\n",
    "locations_gdf = gpd.GeoDataFrame(acceptable_df, geometry = 'geometry', crs = {'init':'epsg:4326'})\n",
    "locations_container = AggressiveSpatialIntersect(locations_gdf, merged_dists)\n",
    "\n",
    "for key in origin_container.keys():\n",
    "    origins = origin_container[key]\n",
    "    origin_subset = grid.copy()\n",
    "    origin_subset = origin_subset.loc[origins]\n",
    "    locations = locations_gdf.loc[locations_container[key]]\n",
    "    if len(locations) < 1:\n",
    "        origin_subset['NN'] = None\n",
    "        origin_subset['NN_dist'] = None\n",
    "        bundle.append(origin_subset)\n",
    "    else:\n",
    "        origin_subset_snapped = gn.pandana_snap_points(origin_subset, \n",
    "                                locations, \n",
    "                                source_crs = 'epsg:4326', \n",
    "                                target_crs = 'epsg:32638', \n",
    "                                add_dist_to_node_col = True)\n",
    "        bundle.append(origin_subset_snapped)\n",
    "\n",
    "grid_gdf_adjusted = pd.concat(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid_gdf_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = grid.copy()\n",
    "objs = []\n",
    "if len(Y.loc[Y['NN'].isnull() == True]) > 0:\n",
    "    Y2 = Y.loc[Y['NN'].isnull() == True]\n",
    "    Y2['walkspeed_direct'] = 0\n",
    "    Y2['walk_time_direct'] = 9999999\n",
    "    Y2['NN_dist_direct'] = 9999999\n",
    "    objs.append(Y2)\n",
    "\n",
    "location_elevs = add_elevation(locations_gdf, 'Longitude','Latitude', srtm_pth)\n",
    "Y1 = Y.loc[Y['NN'].isnull() == False]\n",
    "Y1['NN'] = Y1['NN'].astype(int)\n",
    "Y1 = Y1.set_index('NN')\n",
    "Y1['dest_NN_elev'] = location_elevs['point_elev']\n",
    "\n",
    "Y1 = Y1.reset_index()\n",
    "Y1 = generate_walktimes(Y1, start = 'point_elev', end = 'dest_NN_elev', dist = 'NN_dist', max_walkspeed = offroad_speed).reset_index()\n",
    "Y1 = Y1.rename({'walkspeed':'walkspeed_direct', \n",
    "                    'walk_time':'walk_time_direct',\n",
    "                   'NN_dist':'NN_dist_direct'}, axis = 1)\n",
    "objs.append(Y1)\n",
    "\n",
    "grid = pd.concat(objs)\n",
    "\n",
    "grid['PLOT_TIME_SECS'] = grid[['walk_time_direct','total_time_net']].min(axis = 1)\n",
    "grid['PLOT_TIME_MINS'] = grid['PLOT_TIME_SECS'] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burn Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**process complete**\n"
     ]
    }
   ],
   "source": [
    "rst_fn = os.path.join(pth,'pop18_resampled.tif')\n",
    "out_fn = os.path.join(basepth,'output_layers','Round 3','%s.tif' % subset)\n",
    "\n",
    "# Update metadata\n",
    "rst = rt.open(rst_fn, 'r')\n",
    "meta = rst.meta.copy()\n",
    "D_type = rt.float64\n",
    "meta.update(compress='lzw', dtype = D_type, count = 2)\n",
    "\n",
    "with rt.open(out_fn, 'w', **meta) as out:\n",
    "    with rt.open(rst_fn, 'r') as pop:\n",
    "        \n",
    "        # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "        shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "\n",
    "        population = pop.read(1).astype(D_type)\n",
    "        cpy = population.copy()\n",
    "\n",
    "        travel_times = features.rasterize(shapes=shapes, fill=0, out=cpy, transform=out.transform)\n",
    "\n",
    "        out.write_band(1, population)\n",
    "        out.write_band(2, travel_times)\n",
    "print('**process complete**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Zonal Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonalStats(inShp, inRaster, bandNum=1, mask_A = None, reProj = False, minVal = '', maxVal = '', verbose=False , rastType='N', unqVals=[]):\n",
    "    import sys, os, inspect, logging, json\n",
    "    import rasterio, affine\n",
    "\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "\n",
    "    from collections import Counter\n",
    "    from shapely.geometry import box\n",
    "    from affine import Affine\n",
    "    from rasterio import features\n",
    "    from rasterio.mask import mask\n",
    "    from rasterio.features import rasterize\n",
    "    from rasterio.warp import reproject, Resampling\n",
    "    from osgeo import gdal\n",
    "    \n",
    "    ''' Run zonal statistics against an input shapefile\n",
    "    \n",
    "    INPUT VARIABLES\n",
    "    inShp [string or geopandas object] - path to input shapefile\n",
    "    inRaster [string or rasterio object] - path to input raster\n",
    "    \n",
    "    OPTIONAL\n",
    "    bandNum [integer] - band in raster to analyze\n",
    "    reProj [boolean] -  whether to reproject data to match, if not, raise an error\n",
    "    minVal [number] - if defined, will only calculation statistics on values above this number\n",
    "    verbose [boolean] - whether to be loud with responses\n",
    "    rastType [string N or C] - N is numeric and C is categorical. Categorical returns counts of numbers\n",
    "    unqVals [array of numbers] - used in categorical zonal statistics, tabulates all these numbers, will report 0 counts\n",
    "    mask_A [numpy boolean mask] - mask the desired band using an identical shape boolean mask. Useful for doing conditional zonal stats\n",
    "    \n",
    "    RETURNS\n",
    "    array of arrays, one for each feature in inShp\n",
    "    '''   \n",
    "    if isinstance(inShp, str):\n",
    "        inVector = gpd.read_file(inShp) \n",
    "    else:\n",
    "        inVector = inShp\n",
    "    if isinstance(inRaster, str):\n",
    "        curRaster = rasterio.open(inRaster, 'r+')\n",
    "    else:\n",
    "        curRaster = inRaster\n",
    "        \n",
    "    # If mask is not none, apply mask \n",
    "    if mask_A is not None:\n",
    "        \n",
    "        curRaster.write_mask(np.invert(mask_A))\n",
    "    \n",
    "    outputData=[]\n",
    "    if inVector.crs != curRaster.crs:\n",
    "        if reProj:\n",
    "            inVector = inVector.to_crs(curRaster.crs)\n",
    "        else:\n",
    "            raise ValueError(\"Input CRS do not match\")\n",
    "    fCount = 0\n",
    "    tCount = len(inVector['geometry'])\n",
    "    #generate bounding box geometry for raster bbox\n",
    "    b = curRaster.bounds\n",
    "    rBox = box(b[0], b[1], b[2], b[3])\n",
    "    for geometry in inVector['geometry']:\n",
    "        #This test is used in case the geometry extends beyond the edge of the raster\n",
    "        #   I think it is computationally heavy, but I don't know of an easier way to do it\n",
    "        if not rBox.contains(geometry):\n",
    "            geometry = geometry.intersection(rBox)            \n",
    "        try:\n",
    "            fCount = fCount + 1\n",
    "            if fCount % 1000 == 0 and verbose:\n",
    "                tPrint(\"Processing %s of %s\" % (fCount, tCount) )\n",
    "            # get pixel coordinates of the geometry's bounding box\n",
    "            ul = curRaster.index(*geometry.bounds[0:2])\n",
    "            lr = curRaster.index(*geometry.bounds[2:4])\n",
    "            '''\n",
    "            TODO: There is a problem with the indexing - if the shape falls outside the boundaries, it errors\n",
    "                I want to change it to just grab what it can find, but my brain is wrecked and I cannot figure it out\n",
    "            print(geometry.bounds)\n",
    "            print(curRaster.shape)\n",
    "            print(lr)\n",
    "            print(ul)\n",
    "            lr = (max(lr[0], 0), min(lr[1], curRaster.shape[1]))\n",
    "            ul = (min(ul[0], curRaster.shape[0]), min(ul[1]))\n",
    "            '''\n",
    "            # read the subset of the data into a numpy array\n",
    "            window = ((float(lr[0]), float(ul[0]+1)), (float(ul[1]), float(lr[1]+1)))\n",
    "            \n",
    "            if mask is not None:\n",
    "                data = curRaster.read(bandNum, window=window, masked = True)\n",
    "            else:\n",
    "                data = curRaster.read(bandNum, window=window, masked = False)\n",
    "            \n",
    "            # create an affine transform for the subset data\n",
    "            t = curRaster.transform\n",
    "            shifted_affine = Affine(t.a, t.b, t.c+ul[1]*t.a, t.d, t.e, t.f+lr[0]*t.e)\n",
    "\n",
    "            # rasterize the geometry\n",
    "            mask = rasterize(\n",
    "                [(geometry, 0)],\n",
    "                out_shape=data.shape,\n",
    "                transform=shifted_affine,\n",
    "                fill=1,\n",
    "                all_touched=False,\n",
    "                dtype=np.uint8)\n",
    "\n",
    "            # create a masked numpy array\n",
    "            masked_data = np.ma.array(data=data, mask=mask.astype(bool))\n",
    "            if rastType == 'N':                \n",
    "                if minVal != '' or maxVal != '':\n",
    "                    if minVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data < minVal, masked_data)\n",
    "                    if maxVal != '':\n",
    "                        masked_data = np.ma.masked_where(masked_data > maxVal, masked_data)                    \n",
    "                    if masked_data.count() > 0:                        \n",
    "                        results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "                    else :\n",
    "                        results = [-1, -1, -1, -1]                \n",
    "                else:\n",
    "                    results = [masked_data.sum(), masked_data.min(), masked_data.max(), masked_data.mean()]\n",
    "            if rastType == 'C':\n",
    "                if len(unqVals) > 0:                          \n",
    "                    xx = dict(Counter(data.flatten()))\n",
    "                    results = [xx.get(i, 0) for i in unqVals]                \n",
    "                else:\n",
    "                    results = np.unique(masked_data, return_counts=True)                    \n",
    "            outputData.append(results)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            outputData.append([-1, -1, -1, -1])            \n",
    "    return outputData   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\Anaconda3\\envs\\Cfox2\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving national\n",
      "saving district\n"
     ]
    }
   ],
   "source": [
    "if zonal_stats == 0:\n",
    "    pass\n",
    "else:\n",
    "    for resolution in ['national','district']:\n",
    "        out_fn = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers','Round 3',\n",
    "                              '%s.tif' % subset)\n",
    "\n",
    "        sys.path.append(r'C:\\Users\\charl\\Documents\\GitHub\\GOST\\GOSTRocks')\n",
    "\n",
    "        utils = r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\util_files'\n",
    "\n",
    "        yemen_shp_name = os.path.join(utils, r'Yemen_bound.shp')\n",
    "        yemen_shp = gpd.read_file(yemen_shp_name)\n",
    "        \n",
    "        if yemen_shp.crs != {'init': 'epsg:4326'}:\n",
    "            yemen_shp = yemen_shp.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "        district_shp_name = os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\VulnerabilityMatrix', r'VM.shp')\n",
    "        district_shp = gpd.read_file(district_shp_name)\n",
    "        \n",
    "        if district_shp.crs != {'init': 'epsg:4326'}:\n",
    "            district_shp = district_shp.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "        inraster = out_fn\n",
    "        ras = rt.open(inraster, mode = 'r+')\n",
    "        pop = ras.read(1)\n",
    "        tt_matrix = ras.read(2)\n",
    "\n",
    "        if resolution == 'national':\n",
    "            target_shp = yemen_shp\n",
    "        elif resolution == 'district':\n",
    "            target_shp = district_shp\n",
    "\n",
    "        ## First, add on the total population of the district to each district shape\n",
    "\n",
    "        mask_pop = np.ma.masked_where(pop > (200000), pop).mask\n",
    "\n",
    "        base_pop = zonalStats(target_shp, \n",
    "                                inraster, \n",
    "                                bandNum = 1,\n",
    "                                mask_A = mask_pop,\n",
    "                                reProj = False, \n",
    "                                minVal = 0,\n",
    "                                maxVal = np.inf, \n",
    "                                verbose = True, \n",
    "                                rastType='N')\n",
    "\n",
    "        cols = ['total_pop','min','max','mean']\n",
    "\n",
    "        temp_df = pd.DataFrame(base_pop, columns = cols)\n",
    "\n",
    "        target_shp['total_pop'] = temp_df['total_pop']\n",
    "        target_shp['total_pop'].loc[target_shp['total_pop'] == -1] = 0\n",
    "\n",
    "        ## Now, calculate the population within a range of time thresholds from the destination set\n",
    "        for time_thresh in [30,60,120, 240]:\n",
    "\n",
    "            mask_obj = np.ma.masked_where(tt_matrix > (time_thresh), tt_matrix).mask\n",
    "\n",
    "            raw = zonalStats(target_shp, \n",
    "                                inraster, \n",
    "                                bandNum = 1,\n",
    "                                mask_A = mask_obj,\n",
    "                                reProj = False, \n",
    "                                minVal = 0,\n",
    "                                maxVal = np.inf, \n",
    "                                verbose = True, \n",
    "                                rastType='N')\n",
    "\n",
    "            cols = ['pop_%s' % time_thresh,'min','max','mean']\n",
    "\n",
    "            temp_df = pd.DataFrame(raw, columns = cols)\n",
    "\n",
    "            target_shp['pop_%s' % time_thresh] = temp_df['pop_%s' % time_thresh]\n",
    "            target_shp['pop_%s' % time_thresh].loc[target_shp['pop_%s' % time_thresh] == -1] = 0\n",
    "            target_shp['frac_%s' % time_thresh] = (target_shp['pop_%s' % time_thresh]) / (target_shp['total_pop']).fillna(0)\n",
    "            target_shp['frac_%s' % time_thresh].replace([np.inf, -np.inf], 0)\n",
    "            target_shp['frac_%s' % time_thresh] = target_shp['frac_%s' % time_thresh].fillna(0)\n",
    "\n",
    "        # Save to file\n",
    "\n",
    "        if resolution == 'national':\n",
    "            print('saving national')\n",
    "            outter = target_shp[['total_pop','pop_30','frac_30','pop_60','frac_60','pop_120','frac_120','pop_240','frac_240']]\n",
    "            outter.to_csv(os.path.join(basepth, 'output_layers','Round 3','%s_zonal_%s.csv'% (subset, resolution)))\n",
    "        else:\n",
    "            print('saving district')\n",
    "            target_shp['abs_pop_iso'] = target_shp['total_pop'] - target_shp['pop_30']\n",
    "            target_shp.to_file(os.path.join(basepth, 'output_layers','Round 3','%s_zonal_%s.shp' % (subset, resolution)), driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Change Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "test_mode = 'on'\n",
    "if test_mode == 'on':\n",
    "    pass\n",
    "else:\n",
    "    subset = r'PHCs_2018_conflict_delta'\n",
    "    pre_raster = os.path.join(basepth, 'output_layers','Round 3','driving_24th_HERAMS_PHC_NoConflict.tif')\n",
    "    post_raster = os.path.join(basepth, 'output_layers','Round 3','driving_24th_HERAMS_PHCs_ConflictAdj.tif')\n",
    "    out_fn = os.path.join(basepth,'output_layers','Round 3','%s.tif' % subset)\n",
    "\n",
    "    pre = rasterio.open(pre_raster, 'r')\n",
    "    arr_pre = pre.read(2)\n",
    "    post = rasterio.open(post_raster, 'r')\n",
    "    arr_post = post.read(2)\n",
    "    delta = arr_pre - arr_post\n",
    "\n",
    "    # Update metadata\n",
    "    rst_fn = os.path.join(pth,'pop18_resampled.tif')\n",
    "    rst = rasterio.open(rst_fn, 'r')\n",
    "    meta = rst.meta.copy()\n",
    "    D_type = rasterio.float64\n",
    "    meta.update(compress='lzw', dtype = D_type, count = 3)\n",
    "\n",
    "    with rasterio.open(out_fn, 'w', **meta) as out:\n",
    "        with rasterio.open(rst_fn, 'r') as pop:\n",
    "\n",
    "            # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "            #shapes = ((geom,value) for geom, value in zip(grid.geometry, grid.PLOT_TIME_MINS))\n",
    "\n",
    "            population = pop.read(1).astype(D_type)\n",
    "\n",
    "            out.write_band(1, population)\n",
    "            out.write_band(2, delta)\n",
    "            out.write_band(3, delta * population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship Graphs: Vulnerability Component Scores, Accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode == 'on':\n",
    "    pass\n",
    "else:\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    target_shp2 = target_shp.rename({\n",
    "        'Overall Vu':'Overall Vulnerability Level',\n",
    "        'Health Sys':'Health System Capacity Score',\n",
    "        'Hazards':'Hazard Score',\n",
    "        'Impact on':'Impact on Exposed Population Score',\n",
    "        'Food Secur':'Food Security Score',\n",
    "        'Morbidity':'Morbidity Score',\n",
    "        'Nutrition':'Nutrition Score',\n",
    "        'WASH':'WASH Score',\n",
    "        'Social Det':'Social Determinants and Health Outcomes Score',\n",
    "        'total_pop':'Total Population',\n",
    "                                   }, axis = 1)\n",
    "\n",
    "    factors = ['Overall Vulnerability Level','Health System Capacity Score',\n",
    "              'Hazard Score','Impact on Exposed Population Score','Food Security Score',\n",
    "              'WASH Score','Social Determinants and Health Outcomes Score']\n",
    "\n",
    "    fracs = {'frac_30':'30 minutes',\n",
    "            'frac_60': '1 hour',\n",
    "            'frac_120': '2 hours',\n",
    "            'frac_240': '4 hours'}\n",
    "\n",
    "    for groupa in factors:\n",
    "        subg = target_shp2[[groupa,'Total Population','pop_30','pop_60','pop_120','pop_240']].groupby(groupa).sum()\n",
    "        for i in [30, 60, 120, 240]:\n",
    "            subg['frac_%s' % i] = subg['pop_%s' % i] / subg['Total Population']\n",
    "        plotter = subg[['frac_30','frac_60','frac_120','frac_240']]\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(8,4))\n",
    "        title = 'Fraction of Population with access to a HeRAMS hospital for each value \\nof the WHO %s' % (groupa)\n",
    "        pal = sns.cubehelix_palette(4, start=2.7, rot=.1, light = .7, dark=.1)\n",
    "        ax = sns.lineplot(data = plotter, palette = pal, dashes = False).set_title(title)\n",
    "        plt.legend(loc='center right', bbox_to_anchor=(1.2, 0.5), ncol=1)\n",
    "        plt.savefig(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers\\VM_graphs','pop_chart_%s' % groupa), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots: Vulnerability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode == 'on':\n",
    "    pass\n",
    "else:\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    target_shp2 = target_shp.rename({\n",
    "        'Overall Vu':'Overall Vulnerability Level',\n",
    "        'Health Sys':'Health System Capacity Score',\n",
    "        'Hazards':'Hazard Score',\n",
    "        'Impact on':'Impact on Exposed Population Score',\n",
    "        'Food Secur':'Food Security Score',\n",
    "        'Morbidity':'Morbidity Score',\n",
    "        'Nutrition':'Nutrition Score',\n",
    "        'WASH':'WASH Score',\n",
    "        'Social Det':'Social Determinants and Health Outcomes Score',\n",
    "        'total_pop':'Total Population',\n",
    "                                   }, axis = 1)\n",
    "\n",
    "    p = 7\n",
    "\n",
    "    factors = ['Overall Vulnerability Level','Health System Capacity Score',\n",
    "              'Hazard Score','Impact on Exposed Population Score','Food Security Score',\n",
    "              'WASH Score','Social Determinants and Health Outcomes Score']\n",
    "\n",
    "    fracs = {'frac_30':'30 minutes',\n",
    "            'frac_60': '1 hour',\n",
    "            'frac_120': '2 hours',\n",
    "            'frac_240': '4 hours'}\n",
    "\n",
    "    for frac in ['frac_30','frac_60','frac_120','frac_240']:\n",
    "        for groupa in factors:\n",
    "            plt.clf()\n",
    "            plt.figure(figsize=(8,4))\n",
    "            title = 'Fraction of district population living within %s of nearest HeRAMS hospital, \\nbreakdown by WHO %s' % (fracs[frac], groupa)\n",
    "\n",
    "            d = target_shp2[[groupa,frac]]\n",
    "\n",
    "            pal = sns.cubehelix_palette(7, rot=-.5, light = .8, dark=.2)\n",
    "            ax = sns.swarmplot(y = frac, x = groupa, data=d, palette=pal).set_title(title)\n",
    "            ax = sns.boxplot(y = frac, x = groupa, data=d, whis=np.inf, color = \"1\", linewidth = 0.7, dodge = True)\n",
    "            plt.savefig(os.path.join(r'C:\\Users\\charl\\Documents\\GOST\\Yemen\\output_layers\\VM_graphs','Boxplot_%s_%s.png'% (groupa, frac)), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cfox2)",
   "language": "python",
   "name": "cfox2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
